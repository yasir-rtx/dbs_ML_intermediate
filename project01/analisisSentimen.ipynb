{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.regularizers import l2\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import Callback\n",
    "from keras import layers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from collections import Counter\n",
    "from pandas import read_csv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe :\n",
      "                                           Text Clean Sentiment\n",
      "0  kecewa capek capek bunuh musuh headshot bertur...  negative\n",
      "1  plisss pembaruan download ulang seharian downl...  negative\n",
      "2  taii update jam ehh ngeleg musuh didepan mati ...  negative\n",
      "3  bagus sayang gk hp ram rendah tolong update ku...  negative\n",
      "4  login pasword email sinyalnya bagus sudahnya p...  negative\n",
      "\n",
      "Label Spec :\n",
      " Sentiment\n",
      "negative    3418\n",
      "positive    3418\n",
      "neutral     3418\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique words :  20993\n",
      "\n",
      "Encoder Test : \n",
      "Text Original :  pubg mobile aplikasi bagus menyukai aplikasi iniakan main tolong hilangkan bugkarna bug menganggu tolong tencent games\n",
      "Text Encoded  :  [5, 31, 97, 7, 618, 97, 7011, 6, 2, 383, 7012, 4, 1023, 2, 10, 105]\n",
      "Padding Test : \n",
      "Text Ori   :  dear tencent mohon menyesuaikan level pemain game level rendah match bertemu player level profesional mohon level sesuaikan match\n",
      "Text Token :  [280, 10, 13, 1157, 251, 54, 1, 251, 250, 132, 491, 12, 251, 214, 13, 251, 843, 132]\n",
      "Token Pad  :  [ 280   10   13 1157  251   54    1  251  250  132  491   12  251  214\n",
      "   13  251  843  132    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n",
      "\n",
      "Decoder Test : \n",
      "Text Sequence :  [280, 10, 13, 1157, 251, 54, 1, 251, 250, 132, 491, 12, 251, 214, 13, 251, 843, 132]\n",
      "Text Decoded  :  dear tencent mohon menyesuaikan level pemain game level rendah match bertemu player level profesional mohon level sesuaikan match\n"
     ]
    }
   ],
   "source": [
    "# Loading Dataset\n",
    "df = read_csv(\"dataset_pubg.csv\")\n",
    "# print(df.shape, end=\"\\n\\n\")\n",
    "print(\"Dataframe :\\n\", df.head())\n",
    "print(\"\\nLabel Spec :\\n\", df['Sentiment'].value_counts())\n",
    "\n",
    "# Pisahkan atribut dengan label\n",
    "dataset = df['Text Clean'].to_numpy()\n",
    "label = df.Sentiment.to_numpy()\n",
    "\n",
    "# count unique words\n",
    "def counter_word(texts):\n",
    "    count = Counter()\n",
    "    for text in texts.values:\n",
    "        for word in text.split():\n",
    "            count[word] += 1\n",
    "    return count\n",
    "\n",
    "counter = counter_word(df['Text Clean'])\n",
    "num_unique_words = len(counter)     # 20993\n",
    "# counter.most_common(5)\n",
    "print(\"\\nUnique words : \", num_unique_words)\n",
    "\n",
    "# Tokenizing\n",
    "# tokenizing dataset\n",
    "tokenizer = Tokenizer(num_words=num_unique_words)\n",
    "tokenizer.fit_on_texts(dataset)\n",
    "# dictionary for word\n",
    "word_index = tokenizer.word_index\n",
    "# turn dataset to sequence\n",
    "dataset_seq = tokenizer.texts_to_sequences(dataset)\n",
    "print(\"\\nEncoder Test : \")\n",
    "print(\"Text Original : \", dataset[6])\n",
    "print(\"Text Encoded  : \", dataset_seq[6])\n",
    "\n",
    "# Padding\n",
    "max_word_length = 120\n",
    "dataset_padded = pad_sequences(dataset_seq, maxlen=max_word_length, padding=\"post\", truncating=\"post\")\n",
    "print(\"Padding Test : \")\n",
    "print(\"Text Ori   : \", dataset[10])\n",
    "print(\"Text Token : \", dataset_seq[10])\n",
    "print(\"Token Pad  : \", dataset_padded[10])\n",
    "\n",
    "# Decoder\n",
    "# Flip word dictionary (key, value)\n",
    "reverse_word_index = dict([(idx, word) for (word, idx) in word_index.items()])\n",
    "def decode(sequence):\n",
    "    return \" \".join([reverse_word_index.get(idx, \"?\") for idx in sequence])\n",
    "decoded_text = decode(dataset_seq[10])\n",
    "print(\"\\nDecoder Test : \")\n",
    "print(\"Text Sequence : \", dataset_seq[10])\n",
    "print(\"Text Decoded  : \", decoded_text)\n",
    "\n",
    "# One-Hot Encoding for labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "label_one_hot = encoder.fit_transform(label.reshape(-1, 1))\n",
    "\n",
    "# Data Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset_padded, label_one_hot, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') >= 0.92 and logs.get('val_accuracy') >= 0.92):\n",
    "            self.ltm.stop_training=True\n",
    "            print('\\nModel telah mencapai akurasi 92%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skema 2 : LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 120, 120)          2519160   \n",
      "                                                                 \n",
      " bidirectional_4 (Bidirecti  (None, 120, 240)          231360    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_5 (Bidirecti  (None, 120, 128)          156160    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_6 (Bidirecti  (None, 120, 64)           41216     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_7 (Bidirecti  (None, 32)                10368     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                2112      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 16)                1040      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 3)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2961467 (11.30 MB)\n",
      "Trainable params: 2961467 (11.30 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Feature Extraction\n",
    "wordEmbedding = layers.Embedding(\n",
    "                        input_dim=num_unique_words, \n",
    "                        output_dim=120, \n",
    "                        input_length=max_word_length)\n",
    "\n",
    "lstm = Sequential([\n",
    "    wordEmbedding,\n",
    "    layers.Bidirectional(layers.LSTM(120, return_sequences=True, dropout=0.2, kernel_regularizer=l2(0.01))),\n",
    "    # layers.Bidirectional(layers.LSTM(120, return_sequences=True, dropout=0.2, kernel_regularizer=l2(0.01))),\n",
    "    layers.Bidirectional(layers.LSTM(64, return_sequences=True, dropout=0.2, kernel_regularizer=l2(0.01))),\n",
    "    layers.Bidirectional(layers.LSTM(32, return_sequences=True, dropout=0.2, kernel_regularizer=l2(0.01))),\n",
    "    layers.Bidirectional(layers.LSTM(16, dropout=0.2, kernel_regularizer=l2(0.01))),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    # layers.Dense(32, activation='relu'),\n",
    "    # layers.Dropout(0.5),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dropout(0.8),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "1154/1154 [==============================] - 87s 67ms/step - loss: 12.1160 - accuracy: 0.3375 - val_loss: 10.6110 - val_accuracy: 0.3626\n",
      "Epoch 2/500\n",
      "1154/1154 [==============================] - 60s 52ms/step - loss: 9.3412 - accuracy: 0.3354 - val_loss: 8.1668 - val_accuracy: 0.4142\n",
      "Epoch 3/500\n",
      "1154/1154 [==============================] - 58s 50ms/step - loss: 7.1700 - accuracy: 0.3436 - val_loss: 6.2486 - val_accuracy: 0.3918\n",
      "Epoch 4/500\n",
      "1154/1154 [==============================] - 56s 49ms/step - loss: 5.4713 - accuracy: 0.3376 - val_loss: 4.7566 - val_accuracy: 0.4025\n",
      "Epoch 5/500\n",
      "1154/1154 [==============================] - 57s 49ms/step - loss: 4.1623 - accuracy: 0.3429 - val_loss: 3.6211 - val_accuracy: 0.3821\n",
      "Epoch 6/500\n",
      "1154/1154 [==============================] - 57s 49ms/step - loss: 3.1806 - accuracy: 0.3389 - val_loss: 2.7843 - val_accuracy: 0.3606\n",
      "Epoch 7/500\n",
      "1154/1154 [==============================] - 56s 48ms/step - loss: 2.4704 - accuracy: 0.3313 - val_loss: 2.1919 - val_accuracy: 0.3343\n",
      "Epoch 8/500\n",
      "1154/1154 [==============================] - 56s 49ms/step - loss: 1.9782 - accuracy: 0.3340 - val_loss: 1.7915 - val_accuracy: 0.3343\n",
      "Epoch 9/500\n",
      "1154/1154 [==============================] - 56s 49ms/step - loss: 1.6529 - accuracy: 0.3315 - val_loss: 1.5337 - val_accuracy: 0.3343\n",
      "Epoch 10/500\n",
      "1154/1154 [==============================] - 57s 49ms/step - loss: 1.4473 - accuracy: 0.3350 - val_loss: 1.3737 - val_accuracy: 0.3450\n",
      "Epoch 11/500\n",
      "1154/1154 [==============================] - 55s 48ms/step - loss: 1.3206 - accuracy: 0.3291 - val_loss: 1.2750 - val_accuracy: 0.3343\n",
      "Epoch 12/500\n",
      "1154/1154 [==============================] - 56s 49ms/step - loss: 1.2413 - accuracy: 0.3340 - val_loss: 1.2119 - val_accuracy: 0.3343\n",
      "Epoch 13/500\n",
      "1154/1154 [==============================] - 54s 47ms/step - loss: 1.1896 - accuracy: 0.3379 - val_loss: 1.1701 - val_accuracy: 0.3450\n",
      "Epoch 14/500\n",
      "1154/1154 [==============================] - 55s 47ms/step - loss: 1.1555 - accuracy: 0.3385 - val_loss: 1.1427 - val_accuracy: 0.3207\n",
      "Epoch 15/500\n",
      "1154/1154 [==============================] - 57s 49ms/step - loss: 1.1333 - accuracy: 0.3377 - val_loss: 1.1251 - val_accuracy: 0.3207\n",
      "Epoch 16/500\n",
      "1154/1154 [==============================] - 55s 48ms/step - loss: 1.1191 - accuracy: 0.3372 - val_loss: 1.1139 - val_accuracy: 0.3207\n",
      "Epoch 17/500\n",
      "1154/1154 [==============================] - 56s 48ms/step - loss: 1.1100 - accuracy: 0.3344 - val_loss: 1.1067 - val_accuracy: 0.3207\n",
      "Epoch 18/500\n",
      "1154/1154 [==============================] - 57s 49ms/step - loss: 1.1044 - accuracy: 0.3369 - val_loss: 1.1024 - val_accuracy: 0.3207\n",
      "Epoch 19/500\n",
      "1154/1154 [==============================] - 58s 50ms/step - loss: 1.1011 - accuracy: 0.3410 - val_loss: 1.1001 - val_accuracy: 0.3207\n",
      "Epoch 20/500\n",
      "1154/1154 [==============================] - 55s 47ms/step - loss: 1.0995 - accuracy: 0.3259 - val_loss: 1.0991 - val_accuracy: 0.3207\n",
      "Epoch 21/500\n",
      "1154/1154 [==============================] - 57s 49ms/step - loss: 1.0988 - accuracy: 0.3272 - val_loss: 1.0987 - val_accuracy: 0.3207\n",
      "Epoch 22/500\n",
      "1154/1154 [==============================] - 55s 48ms/step - loss: 1.0987 - accuracy: 0.3353 - val_loss: 1.0987 - val_accuracy: 0.3207\n",
      "Epoch 23/500\n",
      "1154/1154 [==============================] - 58s 51ms/step - loss: 1.0986 - accuracy: 0.3356 - val_loss: 1.0987 - val_accuracy: 0.3207\n",
      "Epoch 24/500\n",
      "1154/1154 [==============================] - 56s 48ms/step - loss: 1.0987 - accuracy: 0.3341 - val_loss: 1.0986 - val_accuracy: 0.3207\n",
      "Epoch 25/500\n",
      "1154/1154 [==============================] - 56s 49ms/step - loss: 1.0987 - accuracy: 0.3287 - val_loss: 1.0987 - val_accuracy: 0.3207\n",
      "Epoch 26/500\n",
      "1154/1154 [==============================] - 57s 49ms/step - loss: 1.0986 - accuracy: 0.3315 - val_loss: 1.0987 - val_accuracy: 0.3207\n",
      "Epoch 27/500\n",
      "1154/1154 [==============================] - 57s 49ms/step - loss: 1.0986 - accuracy: 0.3355 - val_loss: 1.0987 - val_accuracy: 0.3207\n",
      "Epoch 28/500\n",
      "1154/1154 [==============================] - 57s 49ms/step - loss: 1.0986 - accuracy: 0.3312 - val_loss: 1.0987 - val_accuracy: 0.3207\n",
      "Epoch 29/500\n",
      "1154/1154 [==============================] - 57s 50ms/step - loss: 1.0987 - accuracy: 0.3321 - val_loss: 1.0987 - val_accuracy: 0.3207\n",
      "Epoch 30/500\n",
      "1154/1154 [==============================] - 56s 48ms/step - loss: 1.0987 - accuracy: 0.3301 - val_loss: 1.0987 - val_accuracy: 0.3207\n",
      "Epoch 31/500\n",
      "1154/1154 [==============================] - 55s 47ms/step - loss: 1.0986 - accuracy: 0.3326 - val_loss: 1.0987 - val_accuracy: 0.3207\n",
      "Epoch 32/500\n",
      "1154/1154 [==============================] - 55s 48ms/step - loss: 1.0986 - accuracy: 0.3326 - val_loss: 1.0987 - val_accuracy: 0.3207\n",
      "Epoch 33/500\n",
      "1154/1154 [==============================] - 55s 47ms/step - loss: 1.0986 - accuracy: 0.3369 - val_loss: 1.0987 - val_accuracy: 0.3207\n",
      "Epoch 34/500\n",
      "1154/1154 [==============================] - 56s 49ms/step - loss: 1.0987 - accuracy: 0.3321 - val_loss: 1.0987 - val_accuracy: 0.3207\n",
      "Epoch 35/500\n",
      "1154/1154 [==============================] - 55s 48ms/step - loss: 1.0986 - accuracy: 0.3337 - val_loss: 1.0987 - val_accuracy: 0.3207\n",
      "Epoch 36/500\n",
      "1154/1154 [==============================] - 54s 47ms/step - loss: 1.0986 - accuracy: 0.3326 - val_loss: 1.0987 - val_accuracy: 0.3207\n",
      "Epoch 37/500\n",
      "1154/1154 [==============================] - 56s 48ms/step - loss: 1.0986 - accuracy: 0.3330 - val_loss: 1.0987 - val_accuracy: 0.3207\n",
      "Epoch 38/500\n",
      "1154/1154 [==============================] - 55s 47ms/step - loss: 1.0986 - accuracy: 0.3399 - val_loss: 1.0987 - val_accuracy: 0.3207\n",
      "Epoch 39/500\n",
      "1154/1154 [==============================] - 55s 48ms/step - loss: 1.0986 - accuracy: 0.3341 - val_loss: 1.0987 - val_accuracy: 0.3207\n",
      "Epoch 40/500\n",
      "1154/1154 [==============================] - 56s 49ms/step - loss: 1.0986 - accuracy: 0.3328 - val_loss: 1.0987 - val_accuracy: 0.3207\n",
      "Epoch 41/500\n",
      "1154/1154 [==============================] - 55s 48ms/step - loss: 1.0986 - accuracy: 0.3367 - val_loss: 1.0987 - val_accuracy: 0.3207\n",
      "Epoch 42/500\n",
      "1154/1154 [==============================] - 55s 48ms/step - loss: 1.0986 - accuracy: 0.3326 - val_loss: 1.0987 - val_accuracy: 0.3207\n",
      "Epoch 43/500\n",
      "1154/1154 [==============================] - 56s 48ms/step - loss: 1.0986 - accuracy: 0.3365 - val_loss: 1.0987 - val_accuracy: 0.3207\n",
      "Epoch 44/500\n",
      "1154/1154 [==============================] - 56s 49ms/step - loss: 1.0986 - accuracy: 0.3355 - val_loss: 1.0987 - val_accuracy: 0.3207\n",
      "Epoch 45/500\n",
      "1154/1154 [==============================] - 56s 48ms/step - loss: 1.0986 - accuracy: 0.3327 - val_loss: 1.0987 - val_accuracy: 0.3207\n",
      "Epoch 46/500\n",
      "1154/1154 [==============================] - 57s 49ms/step - loss: 1.0986 - accuracy: 0.3351 - val_loss: 1.0987 - val_accuracy: 0.3207\n",
      "Epoch 47/500\n",
      "1154/1154 [==============================] - 56s 49ms/step - loss: 1.0986 - accuracy: 0.3344 - val_loss: 1.0987 - val_accuracy: 0.3207\n",
      "Epoch 48/500\n",
      "1154/1154 [==============================] - 56s 49ms/step - loss: 1.0986 - accuracy: 0.3354 - val_loss: 1.0987 - val_accuracy: 0.3207\n",
      "Epoch 49/500\n",
      "1154/1154 [==============================] - 57s 49ms/step - loss: 1.0986 - accuracy: 0.3347 - val_loss: 1.0987 - val_accuracy: 0.3207\n",
      "Epoch 50/500\n",
      "1154/1154 [==============================] - 56s 49ms/step - loss: 1.0986 - accuracy: 0.3353 - val_loss: 1.0987 - val_accuracy: 0.3207\n",
      "Epoch 51/500\n",
      "1154/1154 [==============================] - 57s 49ms/step - loss: 1.0987 - accuracy: 0.3330 - val_loss: 1.0987 - val_accuracy: 0.3207\n",
      "Epoch 52/500\n",
      "1154/1154 [==============================] - 56s 49ms/step - loss: 1.0986 - accuracy: 0.3305 - val_loss: 1.0987 - val_accuracy: 0.3207\n",
      "Epoch 53/500\n",
      "1154/1154 [==============================] - 56s 49ms/step - loss: 1.0986 - accuracy: 0.3356 - val_loss: 1.0987 - val_accuracy: 0.3207\n",
      "Epoch 54/500\n",
      "1154/1154 [==============================] - 55s 48ms/step - loss: 1.0986 - accuracy: 0.3366 - val_loss: 1.0987 - val_accuracy: 0.3207\n",
      "Epoch 55/500\n",
      "1154/1154 [==============================] - 55s 48ms/step - loss: 1.0986 - accuracy: 0.3334 - val_loss: 1.0987 - val_accuracy: 0.3207\n",
      "Epoch 56/500\n",
      "1154/1154 [==============================] - 55s 47ms/step - loss: 1.0986 - accuracy: 0.3365 - val_loss: 1.0987 - val_accuracy: 0.3207\n",
      "Epoch 57/500\n",
      "1154/1154 [==============================] - 56s 48ms/step - loss: 1.0986 - accuracy: 0.3363 - val_loss: 1.0987 - val_accuracy: 0.3207\n",
      "Epoch 58/500\n",
      "1154/1154 [==============================] - 55s 48ms/step - loss: 1.0986 - accuracy: 0.3336 - val_loss: 1.0987 - val_accuracy: 0.3207\n",
      "Epoch 59/500\n",
      "1154/1154 [==============================] - 56s 49ms/step - loss: 1.0986 - accuracy: 0.3388 - val_loss: 1.0987 - val_accuracy: 0.3207\n",
      "Epoch 60/500\n",
      "1154/1154 [==============================] - 56s 49ms/step - loss: 1.0986 - accuracy: 0.3349 - val_loss: 1.0987 - val_accuracy: 0.3207\n",
      "Epoch 61/500\n",
      "1154/1154 [==============================] - 55s 48ms/step - loss: 1.0986 - accuracy: 0.3352 - val_loss: 1.0987 - val_accuracy: 0.3207\n",
      "Epoch 62/500\n",
      "1154/1154 [==============================] - 56s 48ms/step - loss: 1.0986 - accuracy: 0.3424 - val_loss: 1.0987 - val_accuracy: 0.3207\n",
      "Epoch 63/500\n",
      "1154/1154 [==============================] - 57s 49ms/step - loss: 1.0986 - accuracy: 0.3317 - val_loss: 1.0987 - val_accuracy: 0.3207\n",
      "Epoch 64/500\n",
      "1154/1154 [==============================] - 55s 48ms/step - loss: 1.0986 - accuracy: 0.3349 - val_loss: 1.0987 - val_accuracy: 0.3207\n",
      "Epoch 65/500\n",
      "   1/1154 [..............................] - ETA: 48s - loss: 1.0964 - accuracy: 0.5000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m lstm\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39mcategorical_crossentropy,\n\u001b[1;32m      2\u001b[0m              optimizer\u001b[38;5;241m=\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.00001\u001b[39m),\n\u001b[1;32m      3\u001b[0m              metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 5\u001b[0m history_lstm \u001b[38;5;241m=\u001b[39m \u001b[43mlstm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmyCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/dbs/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.pyenv/dbs/lib/python3.10/site-packages/keras/src/engine/training.py:1733\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1731\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_epoch_begin(epoch)\n\u001b[1;32m   1732\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[0;32m-> 1733\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n\u001b[1;32m   1734\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m             epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m             _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m         ):\n\u001b[1;32m   1741\u001b[0m             callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n",
      "File \u001b[0;32m~/.pyenv/dbs/lib/python3.10/site-packages/keras/src/engine/data_adapter.py:1401\u001b[0m, in \u001b[0;36mDataHandler.steps\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insufficient_data:  \u001b[38;5;66;03m# Set by `catch_stop_iteration`.\u001b[39;00m\n\u001b[1;32m   1400\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1401\u001b[0m original_spe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m   1402\u001b[0m can_run_full_execution \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1403\u001b[0m     original_spe \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1405\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_steps \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m original_spe\n\u001b[1;32m   1406\u001b[0m )\n\u001b[1;32m   1408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_run_full_execution:\n",
      "File \u001b[0;32m~/.pyenv/dbs/lib/python3.10/site-packages/tensorflow/python/ops/resource_variable_ops.py:688\u001b[0m, in \u001b[0;36mBaseResourceVariable.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnumpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    687\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 688\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    689\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    690\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy() is only available when eager execution is enabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lstm.compile(loss=categorical_crossentropy,\n",
    "             optimizer=Adam(learning_rate=0.00001),\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "history_lstm = lstm.fit(X_train, y_train,\n",
    "                        validation_data=(X_test, y_test),\n",
    "                        batch_size=8, \n",
    "                        epochs=500, \n",
    "                        callbacks=myCallback())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
