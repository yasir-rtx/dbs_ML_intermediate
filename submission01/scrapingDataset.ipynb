{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "from google_play_scraper import reviews, Sort\n",
    "from pandas import DataFrame\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from nlp_id.lemmatizer import Lemmatizer\n",
    "from requests import get\n",
    "from io import StringIO\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import nltk, re, json\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build slangwords dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # BUILD slangwords dictionary\n",
    "# slangwords_dict = {}\n",
    "# with open('./resource/slangwords.txt', 'r') as file :\n",
    "#     slangwords = file.readlines()\n",
    "#     for slang in slangwords:\n",
    "#         slang = slang.replace('\\n', '')\n",
    "#         slang = slang.split('\\t')\n",
    "#         # print({slang[0]:slang[1]})\n",
    "#         slangwords_dict.update({slang[0]:slang[1]})\n",
    "        \n",
    "# with open('./resource/slangwords_dict.txt', 'w') as file:\n",
    "#     file.write(json.dumps(slangwords_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minecraft_reviews = reviews(\n",
    "    'com.mojang.minecraftpe',\n",
    "    lang = 'id',\n",
    "    country = 'id',\n",
    "    sort = Sort.MOST_RELEVANT,\n",
    "    count = 20000\n",
    ")\n",
    "# Review the data\n",
    "for review in minecraft_reviews[0]:\n",
    "    print(review['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minecraft_df = DataFrame(minecraft_reviews[0])\n",
    "\n",
    "x_review, y_review = minecraft_df.shape\n",
    "print(\"Record :\", x_review, \"baris\")\n",
    "print(\"Field  :\", y_review, \"kolom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minecraft_df.to_csv(\"./dataset/raw_mc_20000.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE SPECIAL CHARACTERS & CASE FOLDING\n",
    "def cleaning(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', str(text)) # menghapus mention\n",
    "    text = re.sub(r'#[A-Za-z0-9]+', '', text) # menghapus hashtag\n",
    "    text = re.sub(r'RT[\\s]', '', text) # menghapus RT\n",
    "    text = re.sub(r\"http\\S+\", '', text) # menghapus link\n",
    "    text = re.sub(r'[0-9]+', '', text) # menghapus angka\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # menghapus karakter selain huruf dan angka\n",
    "    text = text.replace('\\n', ' ') # mengganti baris baru dengan spasi\n",
    "    text = text.translate(str.maketrans('', '', punctuation)) # menghapus semua tanda baca\n",
    "    text = text.strip(' ') # menghapus karakter spasi dari kiri dan kanan teks\n",
    "    return text.lower()\n",
    "\n",
    "# REMOVE SLANG WORDS\n",
    "def fixSlangwords(text):\n",
    "    with open('./resource/slangwords_dict.txt', 'r') as file :\n",
    "        slangwords = json.loads(file.readline())\n",
    "        # print(slangwords['gua'])\n",
    "    \n",
    "    words = text.split()\n",
    "    fixed_words = []\n",
    "    for word in words:\n",
    "        # print(word.lower() in slangwords)\n",
    "        if word.lower() in slangwords:\n",
    "            # print(slangwords[word.lower()])\n",
    "            fixed_words.append(slangwords[word.lower()])\n",
    "        else :\n",
    "            # print(word)\n",
    "            fixed_words.append(word)\n",
    "            \n",
    "        # text = 'gue gua saya @ ambilin'\n",
    "        # print(fixSlangwords(text))\n",
    "    \n",
    "    return ' '.join(fixed_words)\n",
    "\n",
    "# TOKENIZING\n",
    "def tokenizer(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# REMOVE STOPWORDS\n",
    "def stopwordsRemove(text):\n",
    "    with open('./resource/stopwords.txt', 'r') as file:\n",
    "        stopwords = file.readlines()\n",
    "    stopwordsDict =[]\n",
    "    for word in stopwords:\n",
    "        word = word.replace('\\n', '')\n",
    "        stopwordsDict.append(word)\n",
    "    # print(stopwordsDict)\n",
    "    # print('every' in stopwordsDict)\n",
    "    # text = word.tokenize(text)\n",
    "    # return [word not in stopwordsDict for word in text]\n",
    "    \n",
    "    fix_words = []\n",
    "    for txt in text:\n",
    "        if txt not in stopwordsDict:\n",
    "            fix_words.append(txt)\n",
    "    return fix_words\n",
    "    \n",
    "# LEMMATIZING\n",
    "def lemmatizerWord(text):\n",
    "    lemmatizer = Lemmatizer()\n",
    "    return [lemmatizer.lemmatize(word.lower()) for word in text]\n",
    "\n",
    "def lemmatizing(data):\n",
    "    lemmatized_word = []\n",
    "#     len_data = len(data.tolist())\n",
    "#     for i,text in enumerate(data.tolist()):\n",
    "#         lemmatized_word.append(lemmatizerWord(text))\n",
    "        \n",
    "#         progress = (i+1) / len_data * 100\n",
    "#         print(\"Data-{} | Progress: {:.2f}% | => {}\".format(i, progress, lemmatizerWord(text)))\n",
    "#         clear_output(wait=True)\n",
    "    return lemmatized_word\n",
    "\n",
    "# STEMMING\n",
    "def stemmerWord(text):\n",
    "    stemmer = StemmerFactory().create_stemmer()\n",
    "    return [stemmer.stem(word) for word in text]\n",
    "    \n",
    "    # stem = minecraft_df['text_stopword'].tolist()\n",
    "    # stemmed_word = []\n",
    "    # for i in range(len(stem)):\n",
    "    #     stemmed_word.append(lemmatizerWord(stem[i]))\n",
    "    #     print(i+1, lemmatizerWord(stem[i]))\n",
    "    #     print(\"Len array : \", len(stemmed_word))\n",
    "    #     clear_output(wait=True)\n",
    "    # return stemmed_word\n",
    "\n",
    "def toSentence(text):\n",
    "    return ' '.join(word for word in text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Proses Cleaning     : START\")\n",
    "minecraft_df['text_clean'] = minecraft_df.content.apply(cleaning)\n",
    "print(\"Proses Cleaning     : DONE\\n\")\n",
    "\n",
    "print(\"Proses Slangword    : START\")\n",
    "minecraft_df['text_slangwords'] = minecraft_df['text_clean'].apply(fixSlangwords)\n",
    "print(\"Proses Slangword    : DONE\\n\")\n",
    "\n",
    "print(\"Proses Tokenizing   : START\")\n",
    "minecraft_df['text_tokenizingText'] = minecraft_df['text_slangwords'].apply(tokenizer)\n",
    "print(\"Proses Tokenizing   : DONE\\n\")\n",
    "\n",
    "print(\"Proses Stopword     : START\")\n",
    "minecraft_df['text_stopword'] = minecraft_df['text_tokenizingText'].apply(stopwordsRemove)\n",
    "print(\"Proses Stopword     : DONE\\n\")\n",
    "\n",
    "print(\"Proses Lemmatizing     : START\")\n",
    "minecraft_df['text_lemmatizing'] = minecraft_df['text_stopword'].apply(lemmatizerWord)\n",
    "print(\"Proses Lemmatizing     : DONE\\n\")\n",
    "\n",
    "print(\"Proses Final        : START\")\n",
    "minecraft_df['text_akhir'] = minecraft_df['text_lemmatizing'].apply(toSentence)\n",
    "print(\"Proses Final        : DONE\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get(url='https://raw.githubusercontent.com/angelmetanosaa/dataset/main/lexicon_positive.csv')\n",
    "def create_dict(url, dictType):\n",
    "    response = get(url=url)\n",
    "    if response.status_code == 200:\n",
    "        reader = csv.reader(StringIO(response.text), delimiter=',')\n",
    "        if dictType == 'positive':\n",
    "            print(\"Fetching {} lexicon data\".format(dictType))\n",
    "            for row in reader:\n",
    "                lexicon_positive[row[0]] = int(row[1])\n",
    "        else:\n",
    "            print(\"Fetching {} lexicon data\".format(dictType))\n",
    "            for row in reader:\n",
    "                lexicon_negative[row[0]] = int(row[1])\n",
    "    else:\n",
    "        print(\"Failed to fetch lexicon data\")\n",
    "\n",
    "lexicon_positive, lexicon_negative = {}, {}\n",
    "def sentiment_analysis_lexicon_indonesia(text):\n",
    "    score = 0\n",
    "    for word in text:\n",
    "        if (word in lexicon_positive):\n",
    "            score = score + lexicon_positive[word]\n",
    "    for word in text:\n",
    "        if (word in lexicon_negative):\n",
    "            score = score + lexicon_negative[word]\n",
    "    sentiment='' \n",
    "    if (score > 0) :sentiment = 'positive'\n",
    "    elif (score < 0) :sentiment = 'negative'\n",
    "    else: sentiment = 'neutral'\n",
    "    return score, sentiment\n",
    "\n",
    "create_dict('https://raw.githubusercontent.com/angelmetanosaa/dataset/main/lexicon_positive.csv', \"positive\")\n",
    "create_dict('https://raw.githubusercontent.com/angelmetanosaa/dataset/main/lexicon_negative.csv', \"negative\")\n",
    "print(\"\")\n",
    "\n",
    "results = minecraft_df['text_stemming'].apply(sentiment_analysis_lexicon_indonesia)\n",
    "results = list(zip(*results))\n",
    "minecraft_df['polarity_score'] = results[0]\n",
    "minecraft_df['sentiment'] = results[1]\n",
    "print(minecraft_df['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dbs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
